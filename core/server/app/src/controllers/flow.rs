use crate::{
    controllers::flow_config::FLOW_CONFIG,
    database::{Database, DbResponder},
    middlewares,
    models::{Resource, Sale, User},
    scraping::{
        instagram::InstagramPost, notices::{NoticesScraper, Params}, trends::{Data, Trends, TrendsScraper}
    },
};
use actix_web::{
    HttpMessage, HttpRequest, HttpResponse, Responder, Result, error, middleware::from_fn, post,
    web,
};
use regex::Regex;
use rig::{completion::Prompt, providers};
use serde::Deserialize;
use tracing::{error, info, warn};

use crate::nosql::controllers::analytics::{
    AnalyticsRequest, HashtagData, TrendsData, process_all_hashtags,
};
use crate::nosql::{ScrapedPost, save_scraped_data_to_dynamo};
use aws_sdk_dynamodb::types::AttributeValue;

#[derive(Deserialize)]
pub struct FlowRequest {
    resource_id: i32,
}

// üÜï FUNCI√ìN PARA GUARDAR TODOS LOS DATOS SCRAPED
async fn save_all_scraped_data(scraped_data: &Trends) -> Vec<String> {
    let mut saved_hashtags = Vec::new();

    info!("üöÄ Iniciando guardado de TODOS los datos scraped...");

    // üì∏ PROCESAR INSTAGRAM
    info!(
        "üì∏ Procesando {} items de Instagram",
        scraped_data.data.instagram.len()
    );

    for (index, item) in scraped_data.data.instagram.iter().enumerate() {
        // üîß FIX: Manejar el Option correctamente para evitar lifetime issues
        let scraped_posts: Vec<ScrapedPost> = {
            info!(
                "üì∏ Instagram[{}]: {} con {} posts",
                index,
                item.keyword,
                item.posts.len()
            );

            item.posts
                .iter()
                .filter_map(|post| {
                    Some(ScrapedPost {
                        comments: post.comments as i32,
                        followers: Some(post.followers as i32),
                        likes: post.likes as i32,
                        link: post.link.clone(),
                        time: post.time.clone(),
                        members: None,
                        subreddit: None,
                        title: None,
                        vote: None,
                    })
                })
                .collect()
        };

        // üöÄ INTENTAR GUARDAR (incluso si est√° vac√≠o)
        match save_scraped_data_to_dynamo(
            item.keyword.clone(),
            "instagram".to_string(),
            scraped_posts.clone(),
        )
        .await
        {
            Ok(saved) => {
                if saved {
                    saved_hashtags.push(format!("{}:instagram", item.keyword));
                    info!(
                        "‚úÖ Instagram: {} guardado ({} posts)",
                        item.keyword,
                        scraped_posts.len()
                    );
                } else {
                    warn!("‚ö†Ô∏è Instagram: {} NO guardado (posts vac√≠os)", item.keyword);
                }
            }
            Err(e) => {
                error!("‚ùå Instagram: Error guardando {}: {:?}", item.keyword, e);
            }
        }
    }

    // üî¥ PROCESAR REDDIT
    info!(
        "üî¥ Procesando {} items de Reddit",
        scraped_data.data.reddit.len()
    );

    for (index, item) in scraped_data.data.reddit.iter().enumerate() {
        // üîß FIX: Manejar el Option correctamente para evitar lifetime issues
        let scraped_posts: Vec<ScrapedPost> = {
            info!(
                "üî¥ Reddit[{}]: {} con {} posts",
                index,
                item.keyword,
                item.posts.len()
            );

            item.posts
                .iter()
                .filter_map(|post| {
                    Some(ScrapedPost {
                        comments: post.comments as i32,
                        followers: None, // Reddit no tiene followers
                        likes: 0,        // Reddit usa upvotes en vez de likes
                        link: post.title.clone(),
                        time: post.title.clone(),
                        members: Some(post.members as i32),
                        subreddit: Some(post.subreddit.clone()),
                        title: Some(post.title.clone()),
                        vote: Some(post.vote as i32),
                    })
                })
                .collect()
        };

        // üöÄ INTENTAR GUARDAR (incluso si est√° vac√≠o)
        match save_scraped_data_to_dynamo(
            item.keyword.clone(),
            "reddit".to_string(),
            scraped_posts.clone(),
        )
        .await
        {
            Ok(saved) => {
                if saved {
                    saved_hashtags.push(format!("{}:reddit", item.keyword));
                    info!(
                        "‚úÖ Reddit: {} guardado ({} posts)",
                        item.keyword,
                        scraped_posts.len()
                    );
                } else {
                    warn!("‚ö†Ô∏è Reddit: {} NO guardado (posts vac√≠os)", item.keyword);
                }
            }
            Err(e) => {
                error!("‚ùå Reddit: Error guardando {}: {:?}", item.keyword, e);
            }
        }
    }

    // üê¶ PROCESAR TWITTER (si existe)
    // if let Some(twitter_array) = scraped_data
    //     .get("data")
    //     .and_then(|d| d.get("twitter"))
    //     .and_then(|t| t.as_array())
    // {
    //     info!("üê¶ Procesando {} items de Twitter", twitter_array.len());

    //     for (index, item) in twitter_array.iter().enumerate() {
    //         if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
    //             // üîß FIX: Manejar el Option correctamente para evitar lifetime issues
    //             let scraped_posts: Vec<ScrapedPost> = match item
    //                 .get("posts")
    //                 .and_then(|p| p.as_array())
    //             {
    //                 Some(posts_array) => {
    //                     info!(
    //                         "üê¶ Twitter[{}]: {} con {} posts",
    //                         index,
    //                         keyword,
    //                         posts_array.len()
    //                     );

    //                     posts_array
    //                         .iter()
    //                         .filter_map(|post| {
    //                             Some(ScrapedPost {
    //                                 comments: post
    //                                     .get("comments")
    //                                     .and_then(|c| c.as_i64())
    //                                     .unwrap_or(0)
    //                                     as i32,
    //                                 followers: post
    //                                     .get("followers")
    //                                     .and_then(|f| f.as_i64())
    //                                     .map(|f| f as i32),
    //                                 likes: post.get("likes").and_then(|l| l.as_i64()).unwrap_or(0)
    //                                     as i32,
    //                                 link: post
    //                                     .get("link")
    //                                     .and_then(|l| l.as_str())
    //                                     .unwrap_or("")
    //                                     .to_string(),
    //                                 time: post.get("time")?.as_str()?.to_string(),
    //                                 members: None,
    //                                 subreddit: None,
    //                                 title: None,
    //                                 vote: None,
    //                             })
    //                         })
    //                         .collect()
    //                 }
    //                 None => {
    //                     info!("üê¶ Twitter[{}]: {} SIN posts (array vac√≠o)", index, keyword);
    //                     vec![] // Vector vac√≠o si no hay posts
    //                 }
    //             };

    //             // üöÄ INTENTAR GUARDAR
    //             match save_scraped_data_to_dynamo(
    //                 keyword.to_string(),
    //                 "twitter".to_string(),
    //                 scraped_posts.clone(),
    //             )
    //             .await
    //             {
    //                 Ok(saved) => {
    //                     if saved {
    //                         saved_hashtags.push(format!("{}:twitter", keyword));
    //                         info!(
    //                             "‚úÖ Twitter: {} guardado ({} posts)",
    //                             keyword,
    //                             scraped_posts.len()
    //                         );
    //                     } else {
    //                         warn!("‚ö†Ô∏è Twitter: {} NO guardado (posts vac√≠os)", keyword);
    //                     }
    //                 }
    //                 Err(e) => {
    //                     error!("‚ùå Twitter: Error guardando {}: {:?}", keyword, e);
    //                 }
    //             }
    //         }
    //     }
    // }

    info!(
        "üéØ Guardado completado. Total guardados: {}",
        saved_hashtags.len()
    );
    info!("üìã Hashtags guardados: {:?}", saved_hashtags);

    saved_hashtags
}

// üÜï FUNCI√ìN PARA EXTRAER TODOS LOS HASHTAGS DE LOS DATOS SCRAPED
fn extract_all_hashtags_from_scraped_data(scraped_data: &Trends) -> Vec<String> {
    let mut all_hashtags = Vec::new();

    // Extraer de Instagram
    // if let Some(instagram_array) = scraped_data.get("data")
    //     .and_then(|d| d.get("instagram"))
    //     .and_then(|i| i.as_array()) {

    // }

    for item in &scraped_data.data.instagram {
        if !all_hashtags.contains(&item.keyword) {
            all_hashtags.push(item.keyword.clone());
        }
    }

    // Extraer de Reddit
    for item in &scraped_data.data.reddit {
        if !all_hashtags.contains(&item.keyword) {
            all_hashtags.push(item.keyword.clone());
        }
    }

    // if let Some(reddit_array) = scraped_data.get("data")
    //     .and_then(|d| d.get("reddit"))
    //     .and_then(|r| r.as_array()) {

    //     for item in reddit_array {
    //         if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
    //             if !all_hashtags.contains(&keyword.to_string()) {
    //                 all_hashtags.push(keyword.to_string());
    //             }
    //         }
    //     }
    // }

    // Extraer de Twitter
    // if let Some(twitter_array) = scraped_data.get("data")
    //     .and_then(|d| d.get("twitter"))
    //     .and_then(|t| t.as_array()) {

    //     for item in twitter_array {
    //         if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
    //             if !all_hashtags.contains(&keyword.to_string()) {
    //                 all_hashtags.push(keyword.to_string());
    //             }
    //         }
    //     }
    // }

    // for item in scraped_data.data.twitter {
    //     if !all_hashtags.contains(&item.keyword) {
    //         all_hashtags.push(item.keyword);
    //     }
    // }

    info!("üîç Hashtags extra√≠dos de datos scraped: {:?}", all_hashtags);
    all_hashtags
}

async fn enhance_trends_with_fallback(mut trends: Trends, hashtags: &[String]) -> Trends {
    // Si no podemos acceder a DynamoDB, devolver trends original sin modificar
    let config = aws_config::defaults(aws_config::BehaviorVersion::latest())
        .load()
        .await;
    let client = aws_sdk_dynamodb::Client::new(&config);
    let table_name = "trendhash_hashtag_cache";

    // Solo mejorar Instagram y Reddit si est√°n vac√≠os
    // Mejorar Instagram
    for item in trends.data.instagram.iter_mut() {
        // ‚úÖ PATR√ìN CORRECTO: Primero keyword, despu√©s posts
        let keyword_opt = item.keyword.clone();

        if item.posts.is_empty() && hashtags.contains(&keyword_opt) {
            if let Ok(fallback_data) =
                get_fallback_data(&client, table_name, &keyword_opt, "instagram").await
            {
                item.posts = fallback_data;
                warn!("‚úÖ Fallback aplicado para Instagram: {}", keyword_opt);
            }
        }
    }

    // Mejorar Reddit (mismo patr√≥n)
    // if let Some(reddit) = data.get_mut("reddit").and_then(|v| v.as_array_mut()) {
    //     for item in reddit.iter_mut() {
    //         // ‚úÖ PATR√ìN CORRECTO: Primero keyword, despu√©s posts
    //         let keyword_opt = item
    //             .get("keyword")
    //             .and_then(|k| k.as_str())
    //             .map(|s| s.to_string());

    //         if let (Some(keyword), Some(posts)) = (
    //             keyword_opt,
    //             item.get_mut("posts").and_then(|p| p.as_array_mut()),
    //         ) {
    //             if posts.is_empty() && hashtags.contains(&keyword) {
    //                 if let Ok(fallback_data) =
    //                     get_fallback_data(&client, table_name, &keyword, "reddit").await
    //                 {
    //                     if let serde_json::Value::Array(posts_array) = fallback_data {
    //                         *posts = posts_array;
    //                         warn!("‚úÖ Fallback aplicado para Reddit: {}", keyword);
    //                     }
    //                 }
    //             }
    //         }
    //     }
    // }

    trends
}

async fn get_fallback_data(
    client: &aws_sdk_dynamodb::Client,
    table_name: &str,
    hashtag: &str,
    platform: &str,
) -> Result<Vec<InstagramPost>, Box<dyn std::error::Error>> {
    let pk = format!("HASHTAG#{}", hashtag);
    let sk = format!("DATA#{}", platform);

    let result = client
        .get_item()
        .table_name(table_name)
        .key("pk", AttributeValue::S(pk))
        .key("sk", AttributeValue::S(sk))
        .send()
        .await?;

    if let Some(item) = result.item {
        // Buscar en el campo "scraped_posts" que contiene los datos como JSON
        if let Some(AttributeValue::S(scraped_posts_json)) = item.get("scraped_posts") {
            // Deserializar el JSON a ScrapedHashtagData
            if let Ok(scraped_data) = serde_json::from_str::<crate::nosql::ScrapedHashtagData>(scraped_posts_json) {
                // Convertir ScrapedPost a InstagramPost
                let instagram_posts: Vec<InstagramPost> = scraped_data
                    .posts
                    .into_iter()
                    .map(|scraped_post| InstagramPost {
                        likes: scraped_post.likes as u32,
                        comments: scraped_post.comments as u32,
                        link: scraped_post.link,
                        time: scraped_post.time,
                        followers: scraped_post.followers.unwrap_or(0) as u32,
                    })
                    .collect();

                info!("‚úÖ Datos fallback encontrados: {} posts para {} en {}", 
                      instagram_posts.len(), hashtag, platform);
                return Ok(instagram_posts);
            } else {
                warn!("‚ö†Ô∏è Error deserializando datos fallback para {} en {}", hashtag, platform);
            }
        }
        
        // Si no encontr√≥ en "scraped_posts", intentar buscar por el patr√≥n SCRAPED#
        warn!("‚ö†Ô∏è No se encontr√≥ campo 'scraped_posts', intentando b√∫squeda alternativa...");
        
        // Buscar datos con el patr√≥n SCRAPED#platform#timestamp
        let sk_prefix = format!("SCRAPED#{}", platform);
        let query_result = client
            .query()
            .table_name(table_name)
            .key_condition_expression("pk = :pk AND begins_with(sk, :sk_prefix)")
            .expression_attribute_values(":pk", AttributeValue::S(format!("HASHTAG#{}", hashtag)))
            .expression_attribute_values(":sk_prefix", AttributeValue::S(sk_prefix))
            .scan_index_forward(false) // M√°s recientes primero
            .limit(1) // Solo el m√°s reciente
            .send()
            .await?;

        if let Some(items) = query_result.items {
            if let Some(latest_item) = items.first() {
                if let Some(AttributeValue::S(scraped_posts_json)) = latest_item.get("scraped_posts") {
                    if let Ok(scraped_data) = serde_json::from_str::<crate::nosql::ScrapedHashtagData>(scraped_posts_json) {
                        let instagram_posts: Vec<InstagramPost> = scraped_data
                            .posts
                            .into_iter()
                            .map(|scraped_post| InstagramPost {
                                likes: scraped_post.likes as u32,
                                comments: scraped_post.comments as u32,
                                link: scraped_post.link,
                                time: scraped_post.time,
                                followers: scraped_post.followers.unwrap_or(0) as u32,
                            })
                            .collect();

                        info!("‚úÖ Datos fallback alternativos encontrados: {} posts para {} en {}", 
                              instagram_posts.len(), hashtag, platform);
                        return Ok(instagram_posts);
                    }
                }
            }
        }
    }

    info!("‚ùå No se encontraron datos fallback para {} en {}", hashtag, platform);
    Ok(vec![]) // Devolver array vac√≠o si no encuentra 
}

// üÜï FUNCI√ìN PARA PROCESAR CON ANALYTICS
async fn process_trends_with_analytics(
    trends: &serde_json::Value,
    hashtags: &[String],
) -> serde_json::Value {
    // Convertir trends a formato que entiende analytics
    let analytics_request = convert_trends_to_analytics_request(trends, hashtags);

    // Procesar con las f√≥rmulas
    let calculated_metrics = process_all_hashtags(&analytics_request);

    serde_json::json!({
        "hashtags": calculated_metrics,
        "total_hashtags": hashtags.len(),
        "data_source": "backend_calculations",
        "formulas_used": [
            "insta_ratio()", "insta_viral_rate()",
            "reddit_hourly_ratio()", "reddit_viral_rate()",
            "x_interaction_rate()", "x_viral_rate()"
        ]
    })
}

fn convert_trends_to_analytics_request(
    trends: &serde_json::Value,
    hashtags: &[String],
) -> AnalyticsRequest {
    let mut instagram_data = Vec::new();
    let mut reddit_data = Vec::new();
    let mut twitter_data = Vec::new();

    // Extraer datos de Instagram
    if let Some(instagram_array) = trends
        .get("data")
        .and_then(|d| d.get("instagram"))
        .and_then(|i| i.as_array())
    {
        for item in instagram_array {
            if let (Some(keyword), Some(posts)) = (
                item.get("keyword").and_then(|k| k.as_str()),
                item.get("posts").and_then(|p| p.as_array()),
            ) {
                instagram_data.push(HashtagData {
                    keyword: keyword.to_string(),
                    posts: posts.clone(),
                });
            }
        }
    }

    // Extraer datos de Reddit
    if let Some(reddit_array) = trends
        .get("data")
        .and_then(|d| d.get("reddit"))
        .and_then(|r| r.as_array())
    {
        for item in reddit_array {
            if let (Some(keyword), Some(posts)) = (
                item.get("keyword").and_then(|k| k.as_str()),
                item.get("posts").and_then(|p| p.as_array()),
            ) {
                reddit_data.push(HashtagData {
                    keyword: keyword.to_string(),
                    posts: posts.clone(),
                });
            }
        }
    }

    // Extraer datos de Twitter
    if let Some(twitter_array) = trends
        .get("data")
        .and_then(|d| d.get("twitter"))
        .and_then(|t| t.as_array())
    {
        for item in twitter_array {
            if let (Some(keyword), Some(posts)) = (
                item.get("keyword").and_then(|k| k.as_str()),
                item.get("posts").and_then(|p| p.as_array()),
            ) {
                twitter_data.push(HashtagData {
                    keyword: keyword.to_string(),
                    posts: posts.clone(),
                });
            }
        }
    }

    AnalyticsRequest {
        hashtags: hashtags.to_vec(),
        trends: TrendsData {
            instagram: instagram_data,
            reddit: reddit_data,
            twitter: twitter_data,
        },
        sales: vec![],
    }
}

#[post("/generate-prompt")]
async fn generate_prompt_from_flow(
    req: HttpRequest,
    payload: web::Json<FlowRequest>,
) -> Result<impl Responder> {
    let user_id = req
        .extensions()
        .get::<i32>()
        .cloned()
        .ok_or_else(|| error::ErrorUnauthorized("Missing user id"))?;

    let user = User::get_by_id(user_id)
        .await
        .to_web()?
        .ok_or_else(|| error::ErrorNotFound("User not found"))?;

    let resource = Resource::get_by_id(payload.resource_id)
        .await
        .to_web()?
        .ok_or_else(|| error::ErrorNotFound("Resource not found"))?;

    // let sales_url = FLOW_CONFIG.get_sales_url(&format!("resource/{}", payload.resource_id));
    // let http_client = reqwest::Client::new();

    // let sales_response = http_client
    //     .get(&sales_url)
    //     .send()
    //     .await
    //     .map_err(|e| {
    //         warn!("Error fetching sales data: {}", e);
    //         error::ErrorInternalServerError("Failed to get sales data")
    //     })?;

    // let sales_data: serde_json::Value = sales_response.json().await.map_err(|e| {
    //     warn!("Invalid sales response: {}", e);
    //     error::ErrorInternalServerError("Invalid sales response")
    // })?;

    let sales = Database::get_resource_sales(payload.resource_id)
        .await
        .to_web()?;

    let prompt = format!(
        "Me dedico a la industria de {}. Tengo una {} con alcance {} y {} sucursales. Desarrollo mis operaciones en {}. Ofrezco un {} llamado {}. Consiste en: {}, y se asocia con: {}. Por favor escribe una lista de 5 palabras (palabras individuales, no t√©rminos ni frases, separadas con comas) en ingl√©s mi producto (procura no mencionar el nombre de mi producto) y mi empresa para realizar una b√∫squeda de noticias. Que ninguna palabra contenga guiones. Tambi√©n dame 3 hashtags en ingl√©s que hayan sido populares, que pueda buscar en redes sociales y que se relacionen con mi empresa y con mi producto (procura que los hashtags no incluyan el nombre de mi producto). No incluyas m√°s texto en tu respuesta. Al final de la lista y antes de los hashtags, escribe el s√≠mbolo @.",
        user.industry,
        user.company_size,
        user.scope,
        user.num_branches,
        user.locations,
        resource.r_type,
        resource.name,
        resource.description,
        resource.related_words,
    );

    let client = providers::groq::Client::from_env();
    let agent = client
        .agent("deepseek-r1-distill-llama-70b")
        .preamble("You are an expert researcher")
        .build();

    let response = agent.prompt(&prompt).await.map_err(|e| {
        warn!("Error generating chat response: {}", e);
        error::ErrorInternalServerError("Error generating chat response")
    })?;

    let content = response.trim();
    let after_think = content.split("</think>").nth(1).unwrap_or(content).trim();
    let parts: Vec<&str> = after_think.split('@').collect();

    let raw_sentence = parts.get(0).map(|s| s.trim()).unwrap_or("");

    let words: Vec<&str> = raw_sentence
        .split(", ")
        .map(|w| w.trim())
        .filter(|w| !w.is_empty())
        .take(5)
        .collect();

    let sentence = format!("({})", words.join(" OR "));

    let hashtags_block = parts.get(1).map(|s| s.trim()).unwrap_or("");
    let re = Regex::new(r"#\w+").unwrap();

    let hashtags: Vec<String> = re
        .find_iter(hashtags_block)
        .map(|m| {
            m.as_str()
                .strip_prefix('#')
                .unwrap_or(m.as_str())
                .to_string()
        })
        .collect();

    let today = chrono::Utc::now().naive_utc().date();
    let six_months_ago = today
        .checked_sub_signed(chrono::Duration::days(180))
        .unwrap_or(today);

    // let trends_payload = serde_json::json!({
    //     "query": sentence,
    //     "hashtags": hashtags,
    //     "startdatetime": six_months_ago.to_string(),
    //     "enddatetime": today.to_string(),
    //     "language": "English"
    // });

    // let trends_url = FLOW_CONFIG.get_web_url("trends/get-trends");
    // let http_client = reqwest::Client::new();
    // let trends_response = http_client
    //     .post(&trends_url)
    //     .json(&trends_payload)
    //     .send()
    //     .await
    //     .map_err(|e| {
    //         warn!("Error fetching trends: {}", e);
    //         error::ErrorInternalServerError("Failed to get trends")
    //     })?;

    // let trends: serde_json::Value = trends_response.json().await.map_err(|e| {
    //     warn!("Invalid trends response: {}", e);
    //     error::ErrorInternalServerError("Invalid trends response")
    // })?;

    let params = Params::new(sentence.clone(), six_months_ago, today, String::from("English"));
    let trends = TrendsScraper::get_trends_with_hashtags(params, Some(hashtags))
        .await
        .to_web()?;

    let all_hashtags = extract_all_hashtags_from_scraped_data(&trends);
    let saved_hashtags = save_all_scraped_data(&trends).await;
    let hashtags_for_calculations = if all_hashtags.is_empty() {
        vec![]
    } else {
        all_hashtags.clone()
    };

    let enhanced_trends = enhance_trends_with_fallback(trends, &hashtags_for_calculations).await;
    let enhanced_trends_json = serde_json::to_value(&enhanced_trends).unwrap();
    let calculated_results =
        process_trends_with_analytics(&enhanced_trends_json, &hashtags_for_calculations).await;

    Ok(HttpResponse::Ok().json(serde_json::json!({
        "sentence": sentence,
        "resource_name": resource.name,
        "hashtags": hashtags_for_calculations,
        "trends": enhanced_trends,
        "calculated_results": calculated_results,
        "sales": sales,
        "processing": {
            "status": "‚úÖ CALCULATED",
            "message": "Datos procesados con f√≥rmulas backend y guardados en DynamoDB",
            "backend_calculations": true,
            "scraped_data_saved": true,
            "total_saved": saved_hashtags.len(),
            "saved_hashtags": saved_hashtags
        }
    })))
}

//  üß™ ENDPOINT DE PRUEBA SIN AUTENTICACI√ìN
#[post("/test-generate-prompt")]
async fn test_generate_prompt_from_flow(payload: web::Json<FlowRequest>) -> Result<impl Responder> {
    warn!("üß™ USANDO ENDPOINT DE PRUEBA - Solo para testing!");

    // Usar datos hardcodeados para simular usuario y recurso
    let user_id = 1; // Usuario de prueba

    // Simular datos del usuario (normalmente viene de la DB)
    let user_industry = "Music & Instruments";
    let user_company_size = "Small Business";
    let user_scope = "Regional";
    let user_branches = "3";
    let user_locations = "Austin, Dallas, Houston";

    // Simular datos del recurso (normalmente viene de la DB)
    let resource_type = "Product";
    let resource_name = "Stratocaster Electric Guitar";
    let resource_description =
        "High-quality electric guitar with vintage sound and modern playability";
    let resource_related_words = "music, rock, blues, vintage, amplifier";

    // Simular sales data (vac√≠o para prueba)
    let sales_data = serde_json::json!([]);

    let prompt = format!(
        "Me dedico a la industria de {}. Tengo una {} con alcance {} y {} sucursales. Desarrollo mis operaciones en {}. Ofrezco un {} llamado {}. Consiste en: {}, y se asocia con: {}. Por favor escribe una lista de 5 palabras (palabras individuales, no t√©rminos ni frases, separadas con comas) en ingl√©s mi producto (procura no mencionar el nombre de mi producto) y mi empresa para realizar una b√∫squeda de noticias. Que ninguna palabra contenga guiones. Tambi√©n dame 3 hashtags en ingl√©s que hayan sido populares, que pueda buscar en redes sociales y que se relacionen con mi empresa y con mi producto (procura que los hashtags no incluyan el nombre de mi producto). No incluyas m√°s texto en tu respuesta. Al final de la lista y antes de los hashtags, escribe el s√≠mbolo @.",
        user_industry,
        user_company_size,
        user_scope,
        user_branches,
        user_locations,
        resource_type,
        resource_name,
        resource_description,
        resource_related_words,
    );

    let client = providers::groq::Client::from_env();
    let agent = client
        .agent("deepseek-r1-distill-llama-70b")
        .preamble("You are an expert researcher")
        .build();

    let response = agent.prompt(&prompt).await.map_err(|e| {
        warn!("Error generating chat response: {}", e);
        error::ErrorInternalServerError("Error generating chat response")
    })?;

    let content = response.trim();
    let after_think = content.split("</think>").nth(1).unwrap_or(content).trim();
    let parts: Vec<&str> = after_think.split('@').collect();

    let raw_sentence = parts.get(0).map(|s| s.trim()).unwrap_or("");

    let words: Vec<&str> = raw_sentence
        .split(", ")
        .map(|w| w.trim())
        .filter(|w| !w.is_empty())
        .take(5)
        .collect();

    let sentence = format!("({})", words.join(" OR "));

    let hashtags_block = parts.get(1).map(|s| s.trim()).unwrap_or("");
    let re = Regex::new(r"#\w+").unwrap();

    let hashtags: Vec<String> = re
        .find_iter(hashtags_block)
        .map(|m| {
            m.as_str()
                .strip_prefix('#')
                .unwrap_or(m.as_str())
                .to_string()
        })
        .collect();

    let trends = Trends {
        metadata: vec![],
        data: Data {
            instagram: vec![],
            reddit: vec![],
            twitter: vec![],
        },
    };

    // let trends = serde_json::json!({
    //     "data": {
    //         "instagram": [
    //             {
    //                 "keyword": "ElectricGuitar",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "RockMusic",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "VintageGuitars",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Reply",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Mobilelive",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Sita",
    //                 "posts": []
    //             }
    //         ],
    //         "reddit": [
    //             {
    //                 "keyword": "ElectricGuitar",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "RockMusic",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "VintageGuitars",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Reply", // üöÄ INCLUIR DATOS QUE NORMALMENTE NO SE GUARDAN
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Mobilelive",
    //                 "posts": []
    //             },
    //             {
    //                 "keyword": "Sita",
    //                 "posts": []
    //             }
    //         ],
    //         "twitter": []
    //     },
    //     "metadata": []
    // });

    warn!("üî• Usando datos de prueba con todos los hashtags incluidos");

    let all_hashtags = extract_all_hashtags_from_scraped_data(&trends);
    let saved_hashtags = save_all_scraped_data(&trends).await;
    let hashtags_for_calculations = all_hashtags.clone();
    let enhanced_trends = enhance_trends_with_fallback(trends, &hashtags_for_calculations).await;
    let enhanced_trends_json = serde_json::to_value(&enhanced_trends).unwrap();
    let calculated_results =
        process_trends_with_analytics(&enhanced_trends_json, &hashtags_for_calculations).await;

    Ok(HttpResponse::Ok().json(serde_json::json!({
        "status": "üß™ TEST MODE",
        "message": "Endpoint de prueba - datos simulados con TODOS los hashtags",
        "sentence": sentence,
        "hashtags": hashtags_for_calculations, // üÜï USAR TODOS LOS HASHTAGS
        "trends": enhanced_trends,
        "calculated_results": calculated_results,
        "sales": sales_data,
        "debug": {
            "user_id": user_id,
            "resource_id": payload.resource_id,
            "simulated": true,
            "fallback_applied": true,
            "backend_calculations": true,
            "all_hashtags_extracted": all_hashtags,
            "total_saved": saved_hashtags.len(),
            "saved_hashtags": saved_hashtags
        }
    })))
}

#[post("/debug/check-scraped-data")]
async fn debug_check_scraped_data(body: web::Json<serde_json::Value>) -> Result<impl Responder> {
    let scraped_data = body.into_inner();

    info!("üîç Iniciando an√°lisis de datos scraped...");

    let mut debug_info = serde_json::json!({
        "status": "üîç DEBUGGING",
        "found_hashtags": [],
        "instagram_analysis": {},
        "reddit_analysis": {},
        "twitter_analysis": {},
        "summary": {}
    });

    let mut all_found_hashtags = Vec::new();

    if let Some(instagram_array) = scraped_data
        .get("data")
        .and_then(|d| d.get("instagram"))
        .and_then(|i| i.as_array())
    {
        let mut instagram_hashtags = Vec::new();
        for (index, item) in instagram_array.iter().enumerate() {
            if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
                let posts_count = item
                    .get("posts")
                    .and_then(|p| p.as_array())
                    .map(|arr| arr.len())
                    .unwrap_or(0);

                instagram_hashtags.push(serde_json::json!({
                    "index": index,
                    "keyword": keyword,
                    "posts_count": posts_count,
                    "has_posts": posts_count > 0,
                    "will_be_saved": posts_count > 0 // Solo se guardan los que tienen posts
                }));

                if !all_found_hashtags.contains(&keyword.to_string()) {
                    all_found_hashtags.push(keyword.to_string());
                }
            }
        }
        debug_info["instagram_analysis"] = serde_json::json!({
            "total_items": instagram_array.len(),
            "hashtags": instagram_hashtags
        });
    }

    // üî¥ ANALIZAR REDDIT
    if let Some(reddit_array) = scraped_data
        .get("data")
        .and_then(|d| d.get("reddit"))
        .and_then(|r| r.as_array())
    {
        let mut reddit_hashtags = Vec::new();
        for (index, item) in reddit_array.iter().enumerate() {
            if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
                let posts_count = item
                    .get("posts")
                    .and_then(|p| p.as_array())
                    .map(|arr| arr.len())
                    .unwrap_or(0);

                reddit_hashtags.push(serde_json::json!({
                    "index": index,
                    "keyword": keyword,
                    "posts_count": posts_count,
                    "has_posts": posts_count > 0,
                    "will_be_saved": posts_count > 0 // Solo se guardan los que tienen posts
                }));

                if !all_found_hashtags.contains(&keyword.to_string()) {
                    all_found_hashtags.push(keyword.to_string());
                }
            }
        }
        debug_info["reddit_analysis"] = serde_json::json!({
            "total_items": reddit_array.len(),
            "hashtags": reddit_hashtags
        });
    }

    // üê¶ ANALIZAR TWITTER
    if let Some(twitter_array) = scraped_data
        .get("data")
        .and_then(|d| d.get("twitter"))
        .and_then(|t| t.as_array())
    {
        let mut twitter_hashtags = Vec::new();
        for (index, item) in twitter_array.iter().enumerate() {
            if let Some(keyword) = item.get("keyword").and_then(|k| k.as_str()) {
                let posts_count = item
                    .get("posts")
                    .and_then(|p| p.as_array())
                    .map(|arr| arr.len())
                    .unwrap_or(0);

                twitter_hashtags.push(serde_json::json!({
                    "index": index,
                    "keyword": keyword,
                    "posts_count": posts_count,
                    "has_posts": posts_count > 0,
                    "will_be_saved": posts_count > 0
                }));

                if !all_found_hashtags.contains(&keyword.to_string()) {
                    all_found_hashtags.push(keyword.to_string());
                }
            }
        }
        debug_info["twitter_analysis"] = serde_json::json!({
            "total_items": twitter_array.len(),
            "hashtags": twitter_hashtags
        });
    }

    debug_info["found_hashtags"] = serde_json::json!(all_found_hashtags);
    debug_info["summary"] = serde_json::json!({
        "total_unique_hashtags": all_found_hashtags.len(),
        "all_hashtags": all_found_hashtags,
        "analysis_timestamp": chrono::Utc::now().to_rfc3339(),
        "note": "Solo se guardan hashtags que tienen posts (posts_count > 0)"
    });

    info!(
        "üéØ An√°lisis completado. Hashtags encontrados: {:?}",
        all_found_hashtags
    );

    Ok(HttpResponse::Ok().json(debug_info))
}

#[post("/debug/force-save-empty-hashtags")]
async fn debug_force_save_empty_hashtags(
    body: web::Json<serde_json::Value>,
) -> Result<impl Responder> {
    let scraped_data = body.into_inner();

    info!("üöÄ FORZANDO guardado de hashtags vac√≠os para testing...");

    let mut force_saved = Vec::new();

    // Obtener todos los hashtags √∫nicos  
    let trends_from_json: Trends = serde_json::from_value(scraped_data.clone()).unwrap();
    let all_hashtags = extract_all_hashtags_from_scraped_data(&trends_from_json);

    // Forzar guardado de cada hashtag (incluso vac√≠os)
    for hashtag in &all_hashtags {
        // Instagram
        match save_scraped_data_to_dynamo(
            hashtag.clone(),
            "instagram".to_string(),
            vec![], // Vector vac√≠o
        )
        .await
        {
            Ok(_) => {
                force_saved.push(format!("{}:instagram", hashtag));
                info!("‚úÖ Forzado guardado: {} (Instagram)", hashtag);
            }
            Err(e) => {
                error!("‚ùå Error forzando {}: {:?}", hashtag, e);
            }
        }

        // Reddit
        match save_scraped_data_to_dynamo(
            hashtag.clone(),
            "reddit".to_string(),
            vec![], // Vector vac√≠o
        )
        .await
        {
            Ok(_) => {
                force_saved.push(format!("{}:reddit", hashtag));
                info!("‚úÖ Forzado guardado: {} (Reddit)", hashtag);
            }
            Err(e) => {
                error!("‚ùå Error forzando {}: {:?}", hashtag, e);
            }
        }
    }

    Ok(HttpResponse::Ok().json(serde_json::json!({
        "status": "üöÄ FORCE SAVE COMPLETED",
        "message": "Hashtags vac√≠os guardados forzadamente",
        "all_hashtags_found": all_hashtags,
        "force_saved": force_saved,
        "total_saved": force_saved.len(),
        "note": "Este endpoint fuerza el guardado incluso si los posts est√°n vac√≠os"
    })))
}

pub fn routes() -> actix_web::Scope {
    web::scope("/flow")
        .service(test_generate_prompt_from_flow)
        .service(debug_force_save_empty_hashtags)
        .service(
            web::scope("/secure")
                .wrap(from_fn(middlewares::auth))
                .service(generate_prompt_from_flow),
        )
}
